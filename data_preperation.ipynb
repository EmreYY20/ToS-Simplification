{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the following different Terms of Services will be collected from https://tosdr.org/. Then the data will be processed and relevant data will be extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scraping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve service IDs\n",
    "def get_ids():\n",
    "    id_url = \"https://api.tosdr.org/all-services/v1/\"\n",
    "    r = requests.get(id_url, headers=headers)\n",
    "    j = r.json()\n",
    "    tod_ids = []\n",
    "\n",
    "    # Iterate through services and check if ToS data is already downloaded\n",
    "    for service in j['parameters']['services']:\n",
    "        tos_id = service['id']\n",
    "        file_path = os.path.join(\"data\", \"raw_data\", f\"tos_{tos_id}.json\")\n",
    "\n",
    "        if not os.path.isfile(file_path):\n",
    "            tod_ids.append(tos_id)\n",
    "    return tod_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to download ToS data by ID\n",
    "def download_tos(tos_id):\n",
    "    url = f'https://api.tosdr.org/rest-service/v2/{tos_id}.json'\n",
    "    \n",
    "    # Create a session for making HTTP requests\n",
    "    with requests.Session() as session:\n",
    "        try:\n",
    "            r = session.get(url, headers=headers)\n",
    "            j = r.json()\n",
    "            \n",
    "            # Check for errors and handle if error code 193 is encountered\n",
    "            if j.get('error') == 193:\n",
    "                return None\n",
    "            \n",
    "            directory = 'data/raw_data'\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            # Save the ToS data in a JSON file\n",
    "            with open(f\"{directory}/tos_{tos_id}.json\", 'w') as outfile:\n",
    "                json.dump(j, outfile)\n",
    "            return tos_id\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading ToS {tos_id}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main program execution\n",
    "if __name__ == \"__main__\":\n",
    "    ids = get_ids()\n",
    "\n",
    "    # Use a ThreadPoolExecutor to concurrently download ToS data\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        results = list(executor.map(download_tos, ids))\n",
    "\n",
    "    # Filter and count downloaded ToS data\n",
    "    downloaded_tos = [result for result in results if result is not None]\n",
    "    print(f\"Downloaded {len(downloaded_tos)} ToS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_language_detection import LanguageDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to JSON files containing data\n",
    "path_to_json = 'raw_data/'\n",
    "\n",
    "# get all JSON files in the specified directory\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get a language detector using a specific spaCy model\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector(seed=42)  # We use the seed 42 for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store downloaded terms of services\n",
    "data = []\n",
    "\n",
    "# Load data from each JSON file into the 'data' list\n",
    "for json_file in json_files:\n",
    "    with open(path_to_json + json_file, 'r') as f:\n",
    "        data.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store reviewed terms of services\n",
    "reviewed_terms = []\n",
    "\n",
    "# Filter and collect terms of services that are comprehensively reviewed\n",
    "for doc in data:\n",
    "    if doc['parameters']['is_comprehensively_reviewed'] is True:\n",
    "        reviewed_terms.append(doc)\n",
    "\n",
    "# Create summaries by merging quotes from the reviewed terms of services\n",
    "final_data = []\n",
    "\n",
    "# Load the spaCy model and add a custom language detector pipeline\n",
    "nlp_model = spacy.load('en_core_web_sm')\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp_model.add_pipe('language_detector', last=True)\n",
    "\n",
    "# Process each reviewed term of service\n",
    "for doc in reviewed_terms:\n",
    "    legal_contracts = {}\n",
    "\n",
    "    # Group quotes by document ID\n",
    "    for point in doc['parameters']['points']:\n",
    "        if point['quoteStart'] is not None and point['quoteText'] is not None:\n",
    "            legal_contracts.setdefault(point['document_id'], []).append(point)\n",
    "\n",
    "    # Sort quotes by 'quoteStart'\n",
    "    for doc_id, value in legal_contracts.items():\n",
    "        legal_contracts[doc_id] = sorted(value, key=lambda i: i['quoteStart'])\n",
    "\n",
    "        # Initialize variables to store plain text and summary\n",
    "        plain_text = \"\"\n",
    "        summary = \"\"\n",
    "        \n",
    "        # Concatenate quote text and titles to form plain text and summary\n",
    "        for point in legal_contracts[doc_id]:\n",
    "            plain_text += point['quoteText'] + \" \"\n",
    "            summary += point['title'] + \". \"\n",
    "\n",
    "        # Perform regex preprocessing to remove HTML tags and newline characters\n",
    "        plain_text = re.sub(r\"<[^>]*>\", '', plain_text)\n",
    "        plain_text = re.sub(r\"\\n\", ' ', plain_text)\n",
    "\n",
    "        # Language check using the custom language detector\n",
    "        doc = nlp_model(plain_text)\n",
    "        \n",
    "        # Check if the detected language is English ('en')\n",
    "        if doc._.language['language'] == 'en':\n",
    "            final_data.append([plain_text, summary])\n",
    "\n",
    "# Create a Pandas DataFrame from the processed data\n",
    "df = pd.DataFrame(final_data, columns=['plain_text', 'summary'])\n",
    "\n",
    "# Export the DataFrame to a JSON file in 'records' format with lines\n",
    "df.to_json('data/dataset.json', orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
