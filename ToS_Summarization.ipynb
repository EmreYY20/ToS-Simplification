{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOCyU8egNJGZwL+Kkr09Ptb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmreYY20/ToS-Simplification/blob/main/ToS_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Terms of Service Simplification**"
      ],
      "metadata": {
        "id": "RsX2HEhOkp_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data preperation"
      ],
      "metadata": {
        "id": "yl3LGhRbiuOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following different Terms of Services will be collected from https://tosdr.org/.\n",
        "\n",
        "Then the data will be processed and relevant data will be extracted."
      ],
      "metadata": {
        "id": "eexht0RGiUDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Scraping the data"
      ],
      "metadata": {
        "id": "flW9ruu0ictE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EpZXGUYZiE1p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import concurrent.futures"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299'\n",
        "}"
      ],
      "metadata": {
        "id": "Dp91NFBqieMH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to retrieve service IDs\n",
        "def get_ids():\n",
        "    id_url = \"https://api.tosdr.org/all-services/v1/\"\n",
        "    r = requests.get(id_url, headers=headers)\n",
        "    j = r.json()\n",
        "    tod_ids = []\n",
        "\n",
        "    # Iterate through services and check if ToS data is already downloaded\n",
        "    for service in j['parameters']['services']:\n",
        "        tos_id = service['id']\n",
        "        file_path = os.path.join(\"data\", \"raw_data\", f\"tos_{tos_id}.json\")\n",
        "\n",
        "        if not os.path.isfile(file_path):\n",
        "            tod_ids.append(tos_id)\n",
        "    return tod_ids"
      ],
      "metadata": {
        "id": "UHUyKttyioxJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to download ToS data by ID\n",
        "def download_tos(tos_id):\n",
        "    url = f'https://api.tosdr.org/rest-service/v2/{tos_id}.json'\n",
        "\n",
        "    # Create a session for making HTTP requests\n",
        "    with requests.Session() as session:\n",
        "        try:\n",
        "            r = session.get(url, headers=headers)\n",
        "            j = r.json()\n",
        "\n",
        "            # Check for errors and handle if error code 193 is encountered\n",
        "            if j.get('error') == 193:\n",
        "                return None\n",
        "\n",
        "            directory = 'data/raw_data'\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "\n",
        "            # Save the ToS data in a JSON file\n",
        "            with open(f\"{directory}/tos_{tos_id}.json\", 'w') as outfile:\n",
        "                json.dump(j, outfile)\n",
        "            return tos_id\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading ToS {tos_id}: {e}\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "rvSVBwQZip55"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main program execution\n",
        "if __name__ == \"__main__\":\n",
        "    ids = get_ids()\n",
        "\n",
        "    # Use a ThreadPoolExecutor to concurrently download ToS data\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        results = list(executor.map(download_tos, ids))\n",
        "\n",
        "    # Filter and count downloaded ToS data\n",
        "    downloaded_tos = [result for result in results if result is not None]\n",
        "    print(f\"Downloaded {len(downloaded_tos)} ToS.\")"
      ],
      "metadata": {
        "id": "8empGdoukWGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Processing the data"
      ],
      "metadata": {
        "id": "LaswkRsgi1lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy_language_detection"
      ],
      "metadata": {
        "id": "7ucIz9WXjLh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy_language_detection import LanguageDetector"
      ],
      "metadata": {
        "id": "1b0rHm78i3n0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to JSON files containing data\n",
        "path_to_json = 'data/raw_data/'\n",
        "\n",
        "# get all JSON files in the specified directory\n",
        "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]"
      ],
      "metadata": {
        "id": "6ZDZeIo3jAMe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to get a language detector using a specific spaCy model\n",
        "def get_lang_detector(nlp, name):\n",
        "    return LanguageDetector(seed=42)  # We use the seed 42 for consistency"
      ],
      "metadata": {
        "id": "v2Lq5VYjjQEb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store downloaded terms of services\n",
        "data = []\n",
        "\n",
        "# Load data from each JSON file into the 'data' list\n",
        "for json_file in json_files:\n",
        "    with open(path_to_json + json_file, 'r') as f:\n",
        "        data.append(json.load(f))"
      ],
      "metadata": {
        "id": "BQIeJNE7jTcC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store reviewed terms of services\n",
        "reviewed_terms = []\n",
        "\n",
        "# Filter and collect terms of services that are comprehensively reviewed\n",
        "for doc in data:\n",
        "    if doc['parameters']['is_comprehensively_reviewed'] is True:\n",
        "        reviewed_terms.append(doc)\n",
        "\n",
        "# Create summaries by merging quotes from the reviewed terms of services\n",
        "final_data = []\n",
        "\n",
        "# Load the spaCy model and add a custom language detector pipeline\n",
        "nlp_model = spacy.load('en_core_web_sm')\n",
        "Language.factory(\"language_detector\", func=get_lang_detector)\n",
        "nlp_model.add_pipe('language_detector', last=True)\n",
        "\n",
        "# Process each reviewed term of service\n",
        "for doc in reviewed_terms:\n",
        "    legal_contracts = {}\n",
        "\n",
        "    # Group quotes by document ID\n",
        "    for point in doc['parameters']['points']:\n",
        "        if point['quoteStart'] is not None and point['quoteText'] is not None:\n",
        "            legal_contracts.setdefault(point['document_id'], []).append(point)\n",
        "\n",
        "    # Sort quotes by 'quoteStart'\n",
        "    for doc_id, value in legal_contracts.items():\n",
        "        legal_contracts[doc_id] = sorted(value, key=lambda i: i['quoteStart'])\n",
        "\n",
        "        # Initialize variables to store plain text and summary\n",
        "        plain_text = \"\"\n",
        "        summary = \"\"\n",
        "\n",
        "        # Concatenate quote text and titles to form plain text and summary\n",
        "        for point in legal_contracts[doc_id]:\n",
        "            plain_text += point['quoteText'] + \" \"\n",
        "            summary += point['title'] + \". \"\n",
        "\n",
        "        # Perform regex preprocessing to remove HTML tags and newline characters\n",
        "        plain_text = re.sub(r\"<[^>]*>\", '', plain_text)\n",
        "        plain_text = re.sub(r\"\\n\", ' ', plain_text)\n",
        "\n",
        "        # Language check using the custom language detector\n",
        "        doc = nlp_model(plain_text)\n",
        "\n",
        "        # Check if the detected language is English ('en')\n",
        "        if doc._.language['language'] == 'en':\n",
        "            final_data.append([plain_text, summary])\n",
        "\n",
        "# Create a Pandas DataFrame from the processed data\n",
        "df = pd.DataFrame(final_data, columns=['plain_text', 'summary'])\n",
        "\n",
        "# Export the DataFrame to a JSON file in 'records' format with lines\n",
        "df.to_json('data/dataset.json', orient='records', lines=True)"
      ],
      "metadata": {
        "id": "VrCn90XSjU3y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Understanding the data"
      ],
      "metadata": {
        "id": "YtFEQDlXlamJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l6lp9esGlflL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Fine-Tuning"
      ],
      "metadata": {
        "id": "iOa2dZxelk_6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UcXt_ZsxlqeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Evalutation"
      ],
      "metadata": {
        "id": "Lxhvaby_lrNb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gpq7z9YsltHh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}